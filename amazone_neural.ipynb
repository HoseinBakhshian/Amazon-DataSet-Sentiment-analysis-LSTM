{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9e7uay8C2G9u",
        "EsPpYF5Y5eDV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBakhshian/Amazon-DataSet-Sentiment-analysis-LSTM/blob/main/amazone_neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries and read dataset**"
      ],
      "metadata": {
        "id": "9e7uay8C2G9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_qg6HwSoqrOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a478f11e-0d1a-433c-9e29-fb4200aecba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHHEK4y0z-4S"
      },
      "outputs": [],
      "source": [
        "# Importing essential libraries and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dense\n",
        "from keras.layers import  Embedding, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read dataset from google drive and skip on the bad lines\n",
        "data = pd.read_csv('/content/drive/MyDrive/dataset.txt', sep='\\t', on_bad_lines='skip' )\n",
        "\n",
        "#select a subset of dataset contain mentioned columns below\n",
        "data=data[['review_body','star_rating']]"
      ],
      "metadata": {
        "id": "Op9VqeSDrrrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#determine the sentiment for each 'review_body' and add it to sentiment column\n",
        "data['sentiment']=''\n",
        "data['sentiment']=np.where((data['star_rating'] < 3), 'negetive', data['sentiment'])\n",
        "data['sentiment']=np.where((data['star_rating'] > 3 ), 'positive', data['sentiment'])\n",
        "data['sentiment']=np.where((data['star_rating'] == 3), 'neutral', data['sentiment'])"
      ],
      "metadata": {
        "id": "XJYos09RMRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "0OllnddjsgEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "EsPpYF5Y5eDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_Preprocessing(text):\n",
        "  reviews=[text.lower() for text in text]                      #convert text to lower case\n",
        "  reviews=[re.sub(r'\\d+','',text) for text in reviews]         #remove digits\n",
        "  reviews=[re.sub(r'\\S+@\\S+','',text) for text in reviews]     #remove email\n",
        "  reviews=[re.sub(r'^\\w\\s','',text) for text in reviews]       #remove alphanumeric values\n",
        "  reviews=[re.sub(r'[^A-Za-z]+',' ',text) for text in reviews] #remove non words characters\n",
        "  reviews=[text.strip() for text in reviews]                   #remove extra spaces\n",
        "\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  cleaned_reviews=[]\n",
        "  lem_reviews=[]\n",
        "\n",
        "  for review in reviews:\n",
        "    tokens=[word for word in word_tokenize(review) if not word in stop_words]\n",
        "    cleaned_reviews.append(\" \".join(tokens))\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  for review in cleaned_reviews:\n",
        "    lem_reviews.append(\" \".join(list(map(lemmatizer.lemmatize, word_tokenize(review)))))\n",
        "  return lem_reviews"
      ],
      "metadata": {
        "id": "T3EPWEcTlJ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#execute text_Preprocessing method on 'review_body' column items\n",
        "X = []\n",
        "review_body = list(data['review_body'])\n",
        "for item in review_body:\n",
        "    X.append(text_Preprocessing(item))\n",
        "\n",
        "#Y is like the sentiment column\n",
        "#if item in sentiment column is positive we put one in Y otherwise we put zero in Y\n",
        "Y = data['sentiment']\n",
        "Y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, Y)))"
      ],
      "metadata": {
        "id": "fo_mh1sj1lP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split dataset to tarin and test ==> train=80%  test=20%\n",
        "#random_state=42 means train_test_split output is same each time\n",
        "#X_train and X_test contain 'review_body'\n",
        "#Y_train and Y_test contain sentiments\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "GLZaHuKA1lJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing Embedding layer**"
      ],
      "metadata": {
        "id": "PyFZwW7H56d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer() # initializes an instance of the Tokenizer class\n",
        "word_tokenizer.fit_on_texts(X_train) # builds the vocabulary based on the text in X_train and assigns a unique integer index to each word in the vocab\n",
        "\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train) #Transforms each text in texts to a sequence of integers\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "aewBjYpi1seq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_length = len(word_tokenizer.word_index) + 1 #calculates the vocab size"
      ],
      "metadata": {
        "id": "DQcKNteX1sbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen) # adds padding to the train seq to make them same length\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen) # applies padding using the same parameters as training"
      ],
      "metadata": {
        "id": "PGlyZ9hG1sZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict() # making a dict\n",
        "glove_file = open('/content/drive/MyDrive/a2_glove.6B.100d.txt', encoding=\"utf8\") #opens the file containing the pre-trained\n",
        "\n",
        "for line in glove_file: # reads each line in the glove_file\n",
        "    records = line.split() # splits the current line into a list of values based on whitespace\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')  # converts the remaining elements of the records list into a numerical NumPy array\n",
        "    # float32 =  specify a 32-bit floating-point\n",
        "    embeddings_dictionary [word] = vector_dimensions # assigns the word as the key and vector as the value\n",
        "glove_file.close() # closing the file"
      ],
      "metadata": {
        "id": "wqXKQzT11sWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = zeros((vocab_length, 100)) # creates an embedding matrix with size of vocab_length and 100 and all the cells fulled with 0\n",
        "for word, index in word_tokenizer.word_index.items(): # iterates over each word and its corresponding(متناظر) index\n",
        "    embedding_vector = embeddings_dictionary.get(word) # retrieves the pre-trained embedding vector for the current word\n",
        "    # If the word is present in the embeddings_dictionary, its corresponding embedding vector is assigned to embedding_vector. Otherwise, embedding_vector will be None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector # assigns the embedding vector to the corresponding row in the embedding matrix"
      ],
      "metadata": {
        "id": "VPFkU9X41sVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**"
      ],
      "metadata": {
        "id": "WB4Ig0n73R-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network architecture\n",
        "\n",
        "lstm_model = Sequential()# creates a Sequential model, which allows building stack layers sequentially\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False) #defines an Embedding layer\n",
        "# 100: The dimensionality of the word embeddings.\n",
        "# trainable=False: Freezes the embedding layer so that its weights are not updated during training\n",
        "# weights=[embedding_matrix]: Specifies the pre-trained word embeddings to be used\n",
        "\n",
        "lstm_model.add(embedding_layer) # adds the embedding layer to the model\n",
        "lstm_model.add(LSTM(128))  #reads the input data and outputs 128 features\n",
        "lstm_model.add(Dense(1, activation='sigmoid')) # adds Dense layer"
      ],
      "metadata": {
        "id": "rHjVTZof3WnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compiling\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# loss='binary_crossentropy' --> Used as a loss function for binary classification model\n",
        "# optimizer='adam': Specifies the Adam optimizer\n",
        "# Defines the evaluation metric as accuracy"
      ],
      "metadata": {
        "id": "QGEqcewR3YvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "lstm_model_history = lstm_model.fit(X_train, Y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
        "#a number of samples processed before the model is updated\n",
        "#batch_size=128: #a number of samples processed before the model is updated\n",
        "#epochs=6: Sets the number of times the model will iterate over the entire training dataset\n",
        "#verbose=1: Specifies the level of verbosity during training Setting to 1 --> displays progress bars and training information.\n",
        "#validation_split=0.2: Splits a portion of the training data for validation. In this case, 20% of the training data is reserved for validation, while the remaining 80% is used for traini"
      ],
      "metadata": {
        "id": "3lnohnPO3bMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Model**"
      ],
      "metadata": {
        "id": "TwKENSX81yZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on the Test Set\n",
        "\n",
        "score = lstm_model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"test accuracy: {:.2f}%\".format(score[1]*100))\n"
      ],
      "metadata": {
        "id": "NWT7uYS41y6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Performance Charts\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(lstm_model_history.history['acc']) # Plots the training accuracy and lstm_model_history.history['acc'] contains the accuracy values for each epoch on the training set.\n",
        "plt.plot(lstm_model_history.history['val_acc']) #  Plots the validation accuracy\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left') # Adds 'train' corresponds to the training accuracy, and 'test' corresponds to the validation accuracy.\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm_model_history.history['loss']) #  Plots the training loss\n",
        "plt.plot(lstm_model_history.history['val_loss']) # Plots the validation loss\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EQFabqU82fuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}